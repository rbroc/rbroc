---
title: "Bidirectional transformers for context-aware text encoding"
description: "My first DL engineering project (and the project I am currently most excited about!) focuses on training transformer encoders whose representations incorporate information about higher-order context, i.e., characteristics of the author and of the pragmatic context (e.g., the overall topic of the conversation it is embedded in). We feed models a number of text sequences (a target sequence + contexts, i.e., text from either the same author, or from the same subreddit), and train models on a variant of MLM where the MLM head is fed a combination (via sum, concatenation, or attention) between an aggregate representation of the contexts with token-level representations from the target sequence.   \n  \n
I experimented with three DistilBERT-inspired architectures: a bi-encoder (where context and target are fed to two separate encoders, then their representations are concatenated), a 'batch' encoder, a single DistilBERT encoder where contexts are aggregated and concatenated to target sequences, and a hierarchical encoder, where attention is applied across [CLS] tokens in between standard transformer layers to integrate contextual information.
The benefit of this training protocol is evaluated both by comparing their MLM performance compared to traditional MLM training and to a random-context training, and on a triplet-loss author discrimination task.  \n
We also experiment with selective masking of attention heads based on the type of context provided, so to produce separable context representrations at deployment.  \n  \n
The importance of this project is two-fold. First, this way of tuning models to produce context-aware representations may provide intrinsic advantages in NLP tasks, without substantial gains in model complexity. Secondly, text-based representations of text authors could be used to predict individual traits, following the intuition that linguistic behavior is systematically influenced by e.g., personality, experiences, etc.  \n  \n
This project is still in progress, and it has so far been an invaluable source of hands-on learning. I built a large-scale Reddit dataset from scratch - a big data experience that made me proficient in SQL. I have also become increasingly proficient in Tensorflow for both model engineering, distributed training, and efficient data pipelines. Needless to say, being a self-taught DL engineer, I have made all possible rookie mistakes in the process and, in hindsight, have greatly benefitted from each of them :)  \n  \n
The project started as a collaboration with Tal Yarkoni, but I have been entirely responsible for implementation throughout and for its evolution in the last months. The project would greatly benefit from additional expertise, so do ping me if it sounds interesting and you'd be willing to chip in! I am currently training models on our 4 8Gb GPUs, which intrinsically limits training speed and size, but I am hoping to write up a results report either in preprint or as a conference submission in the coming months.  \n
Code is publicly available here: https://github.com/rbroc/personality_reddit. Planning on making everythin tidy and nice once write-up is closer, and to sahre the dataset for public use."
repo: "personality_reddit"
tags: ["NLP", "transformers", "DistilBERT", "TensorFlow", "huggingface", "ML"]
weight: 2
draft: false
---
