---
title: "Bidirectional transformers for context-aware text encoding"
description: "My first engineering project (and the project I am currently most excited about!) focuses on training transformer encoders whose representations incorporate information about higher-order context, i.e., characteristics of the author and/or the pragmatic context. We feed models a target sequence and a number of 'context' sequences (i.e., text from the same author, or from the same subreddit) as a single example, and train models on a variant of MLM where the MLM head is fed the combination ( sum, concatenation, or attention) of token-level representations from the target sequence and an aggregate representation of the contexts. \n  \n
We experiment with three DistilBERT-inspired architectures: a bi-encoder (where context and target are fed to two separate encoders), a 'batch' encoder (single encoder with added context aggregation and target-context combination layers) and a hierarchical encoder (applying attention across [CLS] tokens in between standard transformer layers to integrate information across contexts and target sequence). The benefits of this training protocol are evaluated both by comparing their MLM performance with no-context MLM training and to random-context training, as well as on a triplet-loss author/subreddit discrimination task. We also experiment with selective masking of attention heads based on the type of context provided to simultaneously produce separable context representations.  \n  \n 
The importance of this project is two-fold. First, this way of tuning models to produce context-aware representations may provide intrinsic advantages in NLP tasks (e.g., for MLM, knowledge of the author may make it easier to accurately reconstruct missing words), without substantial increase in model complexity. Secondly, text-based representations of text authors could be used to predict individual traits, following the intuition that linguistic behavior is systematically influenced by personality, experiences, etc.  \n  \n
This project is still in progress, and it has so far been a great source of learning and of motivation to keep working with deep learning and NLP. I had to build a large-scale Reddit dataset from scratch, a 'big data' experience that gained me proficiency in SQL, and I have become increasingly proficient in Tensorflow for both model engineering, distributed training, and efficient data pipelines. Being a self-taught ML engineer, I have made all possible rookie mistakes in the process and have greatly benefitted from each of them.  \n  \n
The project started as a collaboration with Tal Yarkoni, but I have been responsible for implementation throughout and for its evolution in the last months. \n
Code is publicly available [in this repository](https://github.com/rbroc/personality_reddit). Planning on making everything nice and tidy once write-up is close, and to share the dataset for public use."
repo: "personality_reddit"
tags: ["NLP", "transformers", "DistilBERT", "TensorFlow", "huggingface", "ML"]
weight: 2
draft: false
---
